{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54e1c406",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Backprop\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow\n",
    "- SGD\n",
    "- MLP\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the backpropogation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca83085",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Review: MLP\n",
    "\n",
    "- The multilayer perceptron is built up from sequential application of Dense network layers\n",
    "- Each dense layer has the form $$D_l(x; W, b) = W x + b$$\n",
    "- Following each layer we apply a non-linear activation function\n",
    "- Model can be expressed using function composition notation: $$\\hat{y} = \\left(f_{\\text{out}} \\circ D_L \\circ \\cdots \\circ f_2 \\circ D_2 \\circ f_1 \\circ D_1 \\right)(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e574b2b8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Some Notation\n",
    "\n",
    "- Let's define a little bit more notation...\n",
    "- Let the neuron vector for layer $l$ **before** the activation is applied be called $z_l$\n",
    "- Let the neuron's value for layer $l$ **after** activations be $a_l = f_l(z_l)$\n",
    "- Then, we have $$\\begin{aligned}a_0 &= x \\\\ z_l &= W_l a_l + b_l \\; \\text{for }\\; l \\ge 1 \\\\ a_l &= f_l(z_l) \\; \\text{for } \\; l \\ge 1\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e875697f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### How to find $W$ and $b$?\n",
    "\n",
    "- We've relied on keras to `.fit` the models for us\n",
    "- This means keras is able to solve the inverse problem and find values for all $W$ and $b$ tensors such that $\\hat{y} \\approx y$\n",
    "- Keras has used variants of stochastic gradient descent to do this, but how..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "358a943c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Backpropogation: How gradient descent optimizes $W$, $b$\n",
    "\n",
    "- Let's dig deeper into how the weights and biases of our MLP are fit\n",
    "- Let $l$ be the loss function\n",
    "- In principle, we know that we are using gradient descent: $$\\theta' = \\theta - \\alpha \\frac{\\partial l(y,\\hat{y})}{\\partial \\theta}$$\n",
    "- But what actually goes in to $\\frac{\\partial l}{\\partial \\theta}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "033e8869",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Video\n",
    "\n",
    "- Let's watch a video together about how this works\n",
    "- It is from the *excellent* 3Blue1Brown YouTube channel\n",
    "- Thanks to the producer and check out the other videos on the channel\n",
    "\n",
    "\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tIeHLnjs5U8\" title=\"YouTube video player\" frameborder=\"0\" allow=\"accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture\" allowfullscreen></iframe>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b566976",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "\n",
    "YouTubeVideo(\"tIeHLnjs5U8\", width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9acf475b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Takeaways\n",
    "\n",
    "- Understanding the details of backpropogation is not necessary to do effective deep learning, but it sure helps!\n",
    "- Thinking through the backprop process, we gain clarity into some things we've hinted at in previous lectures:\n",
    "    - Saturated activations don't allow information to flow to earlier layers\n",
    "    - Large activations can cause exploding gradients when multiplied in chain rule\n",
    "    - Normalizing input features leads to better behaved neurons early in the network, which impacts stability later\n",
    "    - ReLU can have problem of disappearing gradients -- if activation at later $l$ is negative, ReLU sets to zero and all layers before $l$ have 0 gradient\n",
    "    - Others in this [blog post](https://karpathy.medium.com/yes-you-should-understand-backprop-e2f06eab496b)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01443c6e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### In practice\n",
    "\n",
    "- In practice, we typically allow Tensorflow and its optimizers to handle the backprop for us\n",
    "- However, we should be aware of how it works... it might save us from spending time on network architectures that simply won't work with backprop"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
