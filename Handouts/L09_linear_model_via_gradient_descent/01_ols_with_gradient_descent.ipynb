{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c32b969",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# OLS with Gradient Descent\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow\n",
    "- Linear models\n",
    "- Gradient Descent\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand how gradient descent can be applied to machine learning models\n",
    "- See how to use gradient descent to find parameters of OLS model\n",
    "- Be introduced to `tf.keras` high level API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50ca4e8f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    " \n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers.experimental import preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202ecbce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gradient Descent\n",
    "\n",
    "- First order method for local non-linear optimization\n",
    "- Given a domain $X \\subseteq \\mathbb{R}^D$ and a function $f: X \\rightarrow \\mathbb{R}$\n",
    "- Iterative updates according to gradient ($\\nabla f$) and learning rate ($\\alpha$): $$x' = x - \\alpha \\nabla f(x)$$\n",
    "- Can be applied for arbitrary non-linear optimization problems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a95c0f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Application to ML\n",
    "\n",
    "- Ingredients to supervised learning:\n",
    "    1. Cleaned/transformed data $(x_i, y_i)_{i=1}^N$\n",
    "    2. Choice of model $\\hat{y} = f(x; \\theta)$\n",
    "    3. Choice of loss function $\\ell(y, \\hat{y})$\n",
    "    4. Strategy/algorithm for updating parameters $\\theta$ to minimize $\\ell(y, \\hat{y})$\n",
    "- We will use gradient descent as part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18db196",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Dataset\n",
    "\n",
    "- We'll use a dataset presented in a [tensorflow tutorial](https://www.tensorflow.org/tutorials/keras/regression) \n",
    "- Label: miles per gallon for car\n",
    "- Features: car/engine characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be672919",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data'\n",
    "column_names = ['MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight',\n",
    "                'Acceleration', 'Model Year', 'Origin']\n",
    "\n",
    "raw_dataset = pd.read_csv(url, names=column_names,\n",
    "                          na_values='?', comment='\\t',\n",
    "                          sep=' ', skipinitialspace=True)\n",
    "\n",
    "dataset = raw_dataset.copy().dropna()\n",
    "dataset.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d997ce91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Split and Plot Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46321f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset.sample(frac=0.8, random_state=0)\n",
    "test_dataset = dataset.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06650e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.pairplot(train_dataset[['MPG', 'Cylinders', 'Displacement', 'Weight']], diag_kind='kde')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2cd790",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Separate Features and Targets\n",
    "\n",
    "- We'll try to predict MPG using the other features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8c7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_dataset.copy()\n",
    "test_features = test_dataset.copy()\n",
    "\n",
    "train_labels = train_features.pop('MPG')\n",
    "test_labels = test_features.pop('MPG')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6742e4fc",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Normalization\n",
    "\n",
    "- We now need to normalize the features $X$ to have mean 0 std 1\n",
    "- We'll also convert the targets to `Tensor`s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f90146d",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = preprocessing.Normalization()\n",
    "normalizer.adapt(train_features)\n",
    "X = normalizer(train_features)\n",
    "X_test = normalizer(test_features)\n",
    "\n",
    "# convert y \n",
    "y = tf.convert_to_tensor(train_labels, dtype=\"float32\")\n",
    "y_test = tf.convert_to_tensor(test_labels, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b07e78",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### OLS as Optimization Problem\n",
    "\n",
    "- Let's use least squares on `df` in order to predict the MPG of a car\n",
    "- In doing this, we'll transition to a notation used throughout the ML/DL literature\n",
    "- The model will be: $$\\hat{y}_i = W x_i + b$$\n",
    "    - $W$ is a parameter vector (matrix) called *weights*\n",
    "    - $b$ is a parameter vector called *bias*\n",
    "    - $W$ and $b$ together are the parameters we've been calling $\\theta$\n",
    "- Loss function is MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d4a15b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### OLS via gradient descent\n",
    "\n",
    "- We are now ready to apply gradient descent to the OLS model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a778aaee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_desc_tf_ols(X, y, T=200, alpha=0.1):\n",
    "\n",
    "    # initialize weights and biases\n",
    "    W = tf.Variable(np.random.randn(X.shape[1]).astype(\"float32\"))\n",
    "    b = tf.Variable([0.0])\n",
    "\n",
    "    trace = []\n",
    "    for i in range(T):\n",
    "        # compute loss function\n",
    "        with tf.GradientTape() as tape:\n",
    "            yhat = tf.linalg.matvec(X, W) + b\n",
    "            loss = tf.losses.mse(y, yhat)\n",
    "        \n",
    "        # compute gradients\n",
    "        dW, db = tape.gradient(loss, [W, b])\n",
    "        \n",
    "        # apply update to each parameter using its gradient\n",
    "        Wp = W - alpha * dW\n",
    "        bp = b - alpha * db\n",
    "        \n",
    "        # book keeping\n",
    "        err_theta = max(abs(bp - b), max(abs(Wp - W)))\n",
    "        err = max(abs(db), max(abs(dW)))\n",
    "        status = dict(\n",
    "            i=i,\n",
    "            loss=loss.numpy(),\n",
    "            dW=dW.numpy(),\n",
    "            db=db.numpy(),\n",
    "            err=err.numpy(),\n",
    "            err_theta=err_theta.numpy(),\n",
    "            W=W.numpy(),\n",
    "            b=b.numpy(),\n",
    "        )\n",
    "        trace.append(status)\n",
    "    \n",
    "        # update weights and biases for next iteration\n",
    "        W = tf.Variable(Wp)\n",
    "        b = tf.Variable(bp)\n",
    "\n",
    "    return trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7685410",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "trace = grad_desc_tf_ols(X, y, T=50)\n",
    "pd.DataFrame(trace).plot.scatter(x=\"i\", y=\"loss\", figsize=(10,6));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4048695a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Using Keras\n",
    "\n",
    "- Above we implemented OLS via gradient descent using lower level tensorflow primitives like `tf.linalg.matvec` and `tf.GradientTape()`\n",
    "- This allowed us to build up the algorithm from first principles and control all the computations\n",
    "- Sometimes this level of flexibility is needed, most of the time it isn't\n",
    "- Tensorflow has a built in high-level API called `keras` that provides common modeling building blocks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b9b0ba",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Keras Sequential Models\n",
    "\n",
    "- sklearn pipelines: data processing + modeling steps $\\Rightarrow$ single \"model\"\n",
    "- Separates responsibilities:\n",
    "    - Library (sklearn): apply data transformations, parameters/variables\n",
    "    - Us: describing data flow and model\n",
    "- Keras (and its `Sequential` model) plays a similar role for tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67484636",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### OLS in Keras\n",
    "\n",
    "- To see how keras works, we'll replicate our OLS example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a9f9f9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Model\n",
    "\n",
    "- Recall we used the OLS model $y = W x + b$\n",
    "- This is built into keras as the `Dense` layer\n",
    "- We can combine our `normalizer` and  a `Dense` layer into a `Sequential` model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a88be4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a94657",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Optimizer and Loss\n",
    "\n",
    "- Optimizer\n",
    "    - Keras has a number of gradient descent variants in the `tf.keras.optimizers` package \n",
    "        - We learn about these variants in SGD video!\n",
    "    - Can use `tf.keras.optimizers.SGD` for gradient descent\n",
    "- `.compile`\n",
    "    - The `.compile` method allows us to specifiy an optimizer and a loss\n",
    "    - We'll use SGD and MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0142bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.1),\n",
    "    loss='mean_squared_error' \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6b49d3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Training\n",
    "\n",
    "- Now that the model has been compiled (linked to loss and optimizer), we are ready to train\n",
    "- We use the `.fit` method, which will produce a trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "577c29f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_tf = model.fit(\n",
    "    train_features, train_labels,\n",
    "    batch_size=train_features.shape[0],  # for normal gradient descent\n",
    "    epochs=50,\n",
    "    # suppress logging\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_data=(test_features, test_labels),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edc6f60",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.scatter(np.arange(len(trace_tf.history[\"loss\"])), trace_tf.history[\"loss\"]);\n",
    "pd.DataFrame(trace).plot.scatter(x=\"i\", y=\"loss\", ax=ax, c=\"orange\");\n",
    "ax.legend([\"keras\", \"by hand\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60aae65d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Review: full code\n",
    "\n",
    "- The keras version allowed us to abstract away many of the details\n",
    "- Below we have reproduced all the keras code to see how easy it was:\n",
    "\n",
    "```python\n",
    "# define model\n",
    "model = tf.keras.Sequential([\n",
    "    normalizer,\n",
    "    tf.keras.layers.Dense(1)\n",
    "])\n",
    "\n",
    "# compile: link model to loss and optimizer\n",
    "model.compile(\n",
    "    optimizer=tf.optimizers.SGD(learning_rate=0.1),\n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "\n",
    "# fit model\n",
    "trace_tf = model.fit(\n",
    "    train_features, train_labels,\n",
    "    batch_size=train_features.shape[0],  # for normal gradient descent\n",
    "    epochs=50,\n",
    "    # suppress logging\n",
    "    verbose=0,\n",
    "    # Calculate validation results on 20% of the training data\n",
    "    validation_data=(test_features, test_labels),\n",
    ")\n",
    "```\n",
    "\n",
    "- The benefits we get from using keras will compound as we fit more involved models and/or use more extensive optimization algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa117",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "apple-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
