{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Reinforcement Learning &#x2013; TD learning\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Statistics and Probability\n",
    "- Dynamic Programming\n",
    "- Reinforcement Learning Introduction\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the meaning of the $Q(s, a)$ function\n",
    "- Understand the concept of a temporal difference\n",
    "- Apply temporal differences to form an RL algorithm (Sarsa)\n",
    "\n",
    "**References**\n",
    "\n",
    "- Barto & Sutton book (online by authors [here](http://incompleteideas.net/book/the-book.html)) chapters 4-6\n",
    "- [Stokey and Lucas (1989)](https://www.jstor.org/stable/j.ctvjnrt76) Chapter 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reminder: Dynamic programming\n",
    "\n",
    "- Let's begin by recalling what we know about dynamic programming\n",
    "- Recall the cake eating problem:\n",
    "    - Time is discrete\n",
    "    - $\\beta$ is discount factor\n",
    "    - Size of cake is $\\bar{x}$\n",
    "    - Consumption of cake in period $t$ is $c_t$\n",
    "    - Utility function $u: \\mathbb{R} \\rightarrow \\mathbb{R}$ maps from consumption today into happiness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sequential Problem\n",
    "\n",
    "- Objective\n",
    "$$\\begin{aligned}\n",
    "\\max_{c_t} &\\sum_{t=0}^{\\infty} \\beta^t u(c_t) \\\\\n",
    "\\text{subject to } \\quad & \\sum_{t=0}^{\\infty} c_t \\le \\bar{x} \\\\\n",
    "& c_t \\ge 0 \\quad \\forall t\n",
    "\\end{aligned}\n",
    "$$\n",
    "- Need to solve for *infinite* sequence $\\{c_t\\}_t$\n",
    "- Or..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Value Function\n",
    "\n",
    "- We can set up a value function $$v(\\bar{x}) \\equiv \\sum_{t=0}^{\\infty} \\beta^t u(c_t)$$\n",
    "- $v(\\bar{x})$ is the total *value* the consumer places on having a cake of size $\\bar{x}$\n",
    "- Decompose $v$ into two steps: first period + later periods $$v(\\bar{x}) = \\underbrace{u(c_0)}_{\\text{flow utility}} + \\underbrace{\\beta \\sum_{t=1}^{\\infty} \\beta^{t-1} u(c_t)}_{\\text{continuation utility}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Recursive Formulation\n",
    "\n",
    "- Note: continuation utility depends on $x_{t+1} = x_t - c_t$\n",
    "- Use this observation to write $v: \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ recursively:\n",
    "$$\\begin{aligned}\n",
    "  v(x_t) &= \\max_{0 \\leq c_t  \\leq x} \\underbrace{u(c_t)}_{\\text{flow utility}} + \\underbrace{\\beta v(x_t - c_t)}_{\\text{continuation value}}\n",
    "\\end{aligned}$$\n",
    "- This is known as the **Bellman Equation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Solution to Recursive Problem\n",
    "\n",
    "- A solution to the dynamic program consists of two functions:\n",
    "    1. **Value function** $v^*(x): \\mathbb{R}^+ \\rightarrow \\mathbb{R}$ -- value of beginning period with $x$ cake remaining\n",
    "    2. **Policy function** $c^*(x): \\mathbb{R}^+ \\rightarrow [0, x]$ -- optimal level of consumption with $x$ cake remaining\n",
    "- Under certain regularity conditions (which we assume), the recursive problem (and its solution) is equivalent to the sequential problem we started with"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Connection to RL\n",
    "\n",
    "- How does this connect to RL?\n",
    "- (S, A, R) pattern for RL is very closely related to recursive formulation of dynamic programming\n",
    "    - $S_t \\Longrightarrow x_t$\n",
    "    - $A_t \\Longrightarrow c_t$ \n",
    "    - $R_t \\Longrightarrow u(c_t)$\n",
    "- Expressing $v(x) = \\text{flow utility} + \\text{ continuation value}$ is like repeating (S, A, R) sequence many times\n",
    "- Baseline algorithm for solving DP problem (VFI) is quite similar to how basic RL algorithms work\n",
    "    - Start with guess for value, make decision, update guess, repeat..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## TD-Learning\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Baseline Assumptions\n",
    "\n",
    "- Let state space $\\mathcal{S}$ and action space $\\mathcal{A}$ be discrete\n",
    "- Let $S_t \\in \\mathcal{S}$  represent state at time $t$\n",
    "- Let $A_t \\in \\mathcal{A}(S_t)$ represent action at time $t$\n",
    "- Let $R_{t+1} \\in \\mathcal{R} \\subseteq \\mathbb{R}$ represent reward at time $t+1$\n",
    "- Let state transitions satisfy the Markov property such that \n",
    "$$\n",
    "\\begin{aligned}\n",
    "& p(s', r | s, a) = \\text{Prob}(S_{t+1}=s', R_{t+1}=r | S_{t} = s, A_{t} = 1) \\\\\n",
    "\\text{ where } \\quad & \\sum_{s' \\in \\mathcal{S}} \\sum_{R \\in \\mathcal{R}} p(s', r | s, a) = 1 \\quad \\forall s \\in \\mathcal{S}, a \\in \\mathcal{A}(S) \\\\\n",
    "\\text{ and } \\quad & p(s', r | s, a) \\ge = 0 \\quad \\forall s, s' \\in \\mathcal{S}, a \\in \\mathcal{A}(s), r \\in \\mathcal{R}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "> Note *Markov* means that probability for $S_{t+1}, R_{t+1}$ only depends on $S_t, A_t$ and not and $S_i, A_i, R_i$ where $i < t$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### State Value Function\n",
    "\n",
    "- Let $v^*(s)$ be the optimal value of being in state $s$ (called *state value function*)\n",
    "- We write this as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "v^*(s) &= \\max_{a \\in \\mathcal{A}(s)} E \\left[R_{t+1} + \\beta v^*(S_{t+1}) | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\max_{a \\in \\mathcal{A}(s)} \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta v^*(s') \\right]\n",
    "\\end{aligned}$$\n",
    "- Should be familiar from our dynamic programming studies\n",
    "- Note expectation around the flow utility term $R_{t+1}$, leaving room for that reward to be stochastic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Action Value Function\n",
    "\n",
    "- We can also write an *action value function*\n",
    "- Let $q^*(s, a)$ be the optimal value of being in state $s$ and choosing action $a$:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "q^*(s, a) &= E \\left[R_{t+1} + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(S_{t+1}, a') | S_t=s, A_t=a \\right] \\\\\n",
    "&= \\sum_{s', r} p(s', r | s, a) \\left[ r + \\beta \\max_{a' \\in \\mathcal{A}(S_{t+1})} q^*(s', a') \\right]\n",
    "\\end{aligned}$$\n",
    "- Notice max operator is now *inside* the expectation and applied to future decision $a'$\n",
    "- The function $q^*(s, a)$ is more general than $v^*(s)$: $$v^*(s) = \\max_{a \\in \\mathcal{A}(s)} q^*(s, a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Acting with $q^*$\n",
    "\n",
    "- Goal of RL is to learn to make decisions that maximize $\\sum \\beta^t R_t$\n",
    "- Knowing $q^*(s, a)$ tells us maximium value of being in state $s$ and choosing $a$\n",
    "- If we *knew* $q^*$, acting optimally would be easy: $$a^*(s) = \\text{argmax}_{a\n",
    "           \\in \\mathcal{A}(s)} q^*(s, a)$$\n",
    "- However, we rarely if ever *know* $q^*$, so we must approximate it\n",
    "- We will let $Q(s, a)$ represent our *approximation* of $q^*$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Approximating $q^*$\n",
    "\n",
    "- The goal of TD learning is to find an accurate approximation $Q(s, a)$ such that $Q(s, a) \\approx q^*(s, a) \\; \\forall s, a)$\n",
    "- There are many RL algorithms that seek to do this\n",
    "- We'll focus on two:\n",
    "    - Sarsa\n",
    "    - Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Temporal Differences\n",
    "\n",
    "\n",
    "- The Bellman equaition for our approximation $Q(s, a)$ is $$Q(s,a) = E[R' + \\beta \\max_{a'} Q(s', a') | s,a]$$\n",
    "- Suppose that we interacted with environment and have in hand $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "- Now we plug these into the Bellman by:\n",
    "    1. Using the form of the Bellman\n",
    "    2. But drop $E$ and $\\max$ because we already know the transition that did occur from $(S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1})$\n",
    "    3. Change $=$ an $\\approx$ because this isn't evaluating full Bellman\n",
    "$$Q(S_t,A_t) \\approx R_{t+1} + \\beta  Q(S_{t+1}, A_{t+1})$$\n",
    "- The difference between the left and right and sides is known as a temporal difference: $$TD(0)(Q) \\equiv R_{t+1} + \\beta Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t)$$\n",
    "\n",
    "> There are extensions to the temporal difference that allow for multiple time periods. The 0 in $TD(0)$ indicates that this is *one-step* TD learning. See Chapters 7 and 12 of Sutton/Barto for more info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Learning using TD(0)\n",
    "\n",
    "- We can use temporal differences to *improve* our approximation $Q$:\n",
    "- Let $Q_t(s, a)$ represent our approximation at the start of period $t$\n",
    "- Similar to gradient descent methods, we will take a step from $Q_t$ in a direction that improves its accuracy\n",
    "- To improve accuracy we step in direction of $TD(0)(Q_t)$ (using step size $\\alpha$): \n",
    "$$\n",
    "\\begin{aligned}\n",
    "Q_{t+1}(S_t, A_t) &= Q_t(S_t, A_t) + \\alpha TD(0)(Q_t) \\\\\n",
    "&= Q_t(S_t, A_t) + \\alpha \\left[R_{t+1} + \\beta Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t) \\right]\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Sarsa Algorithm\n",
    "\n",
    "- The Sarsa algorithm applies the update rule we just described\n",
    "- The algorithm is summarized by Barto and Sutton as follows (section 6.4)\n",
    "\n",
    "![sarsa_barto_sutton.png](./sarsa_barto_sutton.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularQ:\n",
    "    def __init__(self):\n",
    "        self.Q = defaultdict(lambda: 0)\n",
    "    \n",
    "    def __call__(self, s, a):\n",
    "        return self.Q[(s.observable_state(), a)]\n",
    "\n",
    "    def __setitem__(self, k, v):\n",
    "        s, a = k\n",
    "        self.Q[(s.observable_state(), a)] = v\n",
    "        \n",
    "    def get_greedy(self, s, A_s):\n",
    "        vals = [self(s, a) for a in A_s]\n",
    "        max_val = max(vals)\n",
    "        return random.choice([a for (a, v) in zip(A_s, vals) if v == max_val])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Sarsa(object):\n",
    "    def __init__(self, environment, epsilon=0.9, alpha=0.1, beta=1.0):\n",
    "        self.env = environment\n",
    "        self.Q = TabularQ()\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        \n",
    "        self.restart_episode()    \n",
    "\n",
    "    def restart_episode(self):\n",
    "        # These will be (S, A) in our notation. Need to initialize\n",
    "        self.s = self.env.reset()\n",
    "        self.a = self.act(self.s, self.env.enumerate_options(self.s))        \n",
    "\n",
    "    def get_greedy(self, s, A_s):\n",
    "        return self.Q.get_greedy(s, A_s)\n",
    "    \n",
    "    def act(self, s, A_s):\n",
    "        if random.random() > self.epsilon:\n",
    "            return random.choice(A_s)\n",
    "        return self.get_greedy(s, A_s)\n",
    "\n",
    "    def done(self, s=None) -> bool:\n",
    "        return self.env.done(s if s else self.s)\n",
    "    \n",
    "    def step(self):\n",
    "        # first take the step (s, a)\n",
    "        s, a = self.s, self.a\n",
    "        sp, r = self.env.step(s, a)\n",
    "        \n",
    "        if self.done(sp):\n",
    "            # game is over\n",
    "            self.s = sp\n",
    "            return\n",
    "        \n",
    "        # then use policy to compute ap\n",
    "        A_sp = self.env.enumerate_options(sp)\n",
    "        ap = self.act(sp, A_sp)\n",
    "        \n",
    "        # now we know S-A-R-S'-A' -- ready to do update\n",
    "        Q, α, β = self.Q, self.alpha, self.beta  # simplify notation\n",
    "        Q[(s, a)] = Q(s, a) + α * (r + β * Q(sp, ap) - Q(s, a))\n",
    "        \n",
    "        # step forward in time\n",
    "        self.s = sp \n",
    "        self.a = ap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: Farkle\n",
    "\n",
    "- In a separate video we implemented the dice game farkle\n",
    "- We'll re-use that code as an environment for `Sarsa` algorithm\n",
    "- For a review of farkle, see video\n",
    "- Today we'll approach it like the RL algorihtm will: \n",
    "    - A stochastic environment that sends states, a list of possible actions, and rewards\n",
    "    - We will *not* specialize based on rules of game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Farkle env\n",
    "\n",
    "- Let's wrap farkle code into environment `Sarsa` expects\n",
    "- Need a few key methods:\n",
    "    - `reset() -> State`\n",
    "    - `enumerate_options(state) -> List[Action]`\n",
    "    - `step(s: State, a: Action) -> Tuple[State, Reward]`\n",
    "    - `done(s:State) -> bool`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import State, Action, RandomFarklePlayer, FarklePlayer, STOP, BANKRUPT\n",
    "from typing import List, Tuple\n",
    "\n",
    "class FarkleEnv:    \n",
    "    # first, some helper methods\n",
    "    def __init__(\n",
    "            self, \n",
    "            opponent: FarklePlayer=RandomFarklePlayer(), \n",
    "            points_to_win=10_000, \n",
    "            verbose: bool = False\n",
    "        ):\n",
    "        self.points_to_win = points_to_win\n",
    "        self.opponent = opponent\n",
    "        self.n_players = 2\n",
    "        self._state = State(self.n_players)\n",
    "        self._history: List[Tuple[State, Action]] = []\n",
    "\n",
    "    @property\n",
    "    def state(self) -> State:\n",
    "        return self._state\n",
    "\n",
    "    def set_state(self, action: Action, new_state: State):\n",
    "        self._history.append((self.state, action))\n",
    "        self._state = new_state\n",
    "    \n",
    "    def opponent_turn(self, s: State) -> State:\n",
    "        choices = s.enumerate_options()         \n",
    "        action = self.opponent.act(s, choices)\n",
    "        sp = s.step(action)\n",
    "\n",
    "        # check if player chose to stop\n",
    "        if sp.current_player != 1:\n",
    "            return sp\n",
    "\n",
    "        # Player didn't stop, but still their turn. Call again\n",
    "        return self.opponent_turn(sp)\n",
    "        \n",
    "    # key methods needed\n",
    "    def done(self, state) -> bool:\n",
    "        return any(score > self.points_to_win for score in state.scores)\n",
    "\n",
    "    def reset(self):\n",
    "        self._state = State(self.n_players)\n",
    "        self._history = []\n",
    "        return self.state.roll()\n",
    "\n",
    "    def step(self, s: State, a: Action) -> Tuple[State, int]:\n",
    "        sp = s.step(a)\n",
    "        r = 0\n",
    "        \n",
    "        # see if we ended\n",
    "        if sp.current_player != 0:\n",
    "            if a is STOP:  \n",
    "                # only score when we choose to stop\n",
    "                r = s.turn_sum\n",
    "            \n",
    "            # take opponent turn\n",
    "            sp = self.opponent_turn(sp)\n",
    "            \n",
    "        self.set_state(a, sp)\n",
    "        return sp, r\n",
    "    \n",
    "    def enumerate_options(self, s: State) -> List[Action]:\n",
    "        return self.state.enumerate_options(s.rolled_dice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Playing Farkle\n",
    "\n",
    "- Let's try it out!\n",
    "- We need to create an env, then pass it to sarsa\n",
    "- We'll also define a helper function to play a game for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = FarkleEnv()\n",
    "sarsa = Sarsa(env, epsilon=0.9, alpha=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def play_game(algo):\n",
    "    algo.restart_episode()\n",
    "    while not algo.done():\n",
    "        algo.step()\n",
    "    return algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(sarsa)\n",
    "sarsa.s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "scores = [h[0].scores for h in env._history]\n",
    "plt.plot(scores)\n",
    "plt.legend([\"sarsa\", \"random\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from farkle import HumanFarklePlayer\n",
    "env_human = FarkleEnv(opponent=HumanFarklePlayer(name=\"Spencer\"), points_to_win=2000)\n",
    "sarsa_human = Sarsa(env_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "play_game(sarsa_human)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sarsa_human.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Learning\n",
    "\n",
    "- Great! Our algorithm can play Farkle\n",
    "- But... it needs to play *many* games to learn how to play well\n",
    "- Let's let it play many more games to build up some intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "def play_many_games(N):\n",
    "    terminal_states = []\n",
    "    print_skip = N // 10\n",
    "    for i in range(N):\n",
    "        play_game(sarsa) \n",
    "        terminal_states.append(sarsa.s)\n",
    "        if i % print_skip == 0:\n",
    "            print(f\"Done with {i}/{N} (len(Q) = {len(sarsa.Q.Q)})\")\n",
    "    return terminal_states\n",
    "\n",
    "# WARNING: this takes a *long time* and requires a lot of ram!\n",
    "# Only use on a computer with at least 32 GB ram\n",
    "# There are ways we could optimize this... such as only including\n",
    "# final score in `terminal_states` and dropping things like current_round\n",
    "# from the state\n",
    "sarsa_history = play_many_games(200000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Did we learn?\n",
    "\n",
    "- Let's analyze the history and see if the algorithm learned with experience"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "won = np.array([s.scores[0] > s.scores[1] for s in sarsa_history])\n",
    "game_idx = np.arange(len(won))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "ax.plot(game_idx, won.cumsum())\n",
    "ax.plot(game_idx, 0.5 * game_idx)\n",
    "plt.legend([\"sarsa\", \"E[random agent]\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "org": null
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
