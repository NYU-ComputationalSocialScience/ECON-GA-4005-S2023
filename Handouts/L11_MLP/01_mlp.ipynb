{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f004290",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Multilayer Perceptron\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow\n",
    "- SGD\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the core concepts behind the Multilayer Perceptron model\n",
    "- Understand the importance of activation functions\n",
    "- Be able to select different output activations for different ML tasks\n",
    "- Be able to define and fit MLP using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "881fb445",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.style.use(\"seaborn\")\n",
    "\n",
    "import tensorflow as tf\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bfaf38b",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Brief Detour: Function Composition\n",
    "\n",
    "- Consider the expression: $$\\hat{y} = f_3(f_2(f_1(x)))$$\n",
    "- This is a *nested* or *recursive* application of functions $f_1$, $f_2$, and $f_3$\n",
    "- A little cumbersome because of all the parenthesis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1141af3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Function Composition Notation\n",
    "\n",
    "- Alternative notation: $$y = (f_3 \\circ f_2 \\circ f_1) (x)$$\n",
    "- This is called *function composition*\n",
    "- Meaning is the same as before:\n",
    "    - Evaluate $f_1(x)$\n",
    "    - Feed output into $f_2$\n",
    "    - Feed that output into $f_3$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20300b5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Comparison in Code\n",
    "\n",
    "- The simplification also carries over into code:\n",
    "\n",
    "```python\n",
    "# nested/recursive form\n",
    "y = f3(f2(f1(x)))\n",
    "\n",
    "# composition\n",
    "fs = [f1, f2, f3]\n",
    "y = x\n",
    "for f in fs:\n",
    "    y = f(y)\n",
    "```\n",
    "- In the composition case we can easiliy apply many functions by adding to list `fs`\n",
    "- Will come up later..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e152ab",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "## Neurons and the MLP model\n",
    "\n",
    "- We will now learn what is meant by \"neural\" in \"neural networks\"\n",
    "- There are three perspectives from which we will look at this model:\n",
    "    1. Visually\n",
    "    2. Mathematically\n",
    "    3. In Code\n",
    "- We'll work through the perspectives in this order\n",
    "- Reccomendation: focus on the perspective that makes the most sense to you!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50cff18d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Visual Perspective\n",
    "\n",
    "- Neural networks are built from neurons\n",
    "- But what are they?\n",
    "- Borrow from biology and attempt to mimic the way our brains work\n",
    "\n",
    "![biology.png](biology.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad9b4c",
   "metadata": {
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Layers of Neurons\n",
    "\n",
    "- The neuron is also called a perceptron\n",
    "- The multi-layer perceptron extends the notion of connecting a network of neurons to one another:\n",
    "    - Neurons are stacked on top of one another in a *layer*\n",
    "    - Layers are stacked in sequence, one after another\n",
    "\n",
    "![MLP.png](MLP.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbddb5e",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Mathematical Perspective\n",
    "\n",
    "- Each neuron in layer $l$ is connected to all neurons in layer $l-1$\n",
    "- Mathematical interpretation of the visuals:\n",
    "    - Squares are input features $x$\n",
    "    - Red circles are numbers (hidden neurons)\n",
    "    - Pink circles are outputs\n",
    "    - Arrows are elements of weight matrix $W$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee1b9be4",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Each Layer is a Linear Regression\n",
    "\n",
    "- Meaning: multilayer perceptron is nested linear regression, separated by non-linear activation function: $$y = f_{\\text{out}}(W_L f_{\\cdots} \\left(W_{\\cdots} f_2(W_2 f_1(W_1 X + b_1) + b_2)  + b_{\\cdots} \\right) + b_{L})$$\n",
    "- that's scary looking... \n",
    "- we'll clean up the notation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39c265d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLP Notation: Activations\n",
    "\n",
    "$$\\begin{aligned}\n",
    "y &= f_{\\text{out}}\\left(W_L f_{\\cdots} \\left(W_{\\cdots} f_2(W_2 \\underbrace{f_1(W_1 X + b_1)}_{\\triangleq a_1} + b_2)  + b_{\\cdots} \\right) + b_{L} \\right) \\\\\n",
    "  &= f_{\\text{out}}\\left(W_L f_{\\cdots} \\left(W_{\\cdots} \\underbrace{f_2(W_2 a_1 + b_2)}_{\\triangleq a_2}  + b_{\\cdots} \\right) + b_{L} \\right) \\\\\n",
    "  &= f_{\\text{out}}\\left(W_L f_{\\cdots} \\left(W_{\\cdots} {a_2}  + b_{\\cdots} \\right) + b_{L} \\right)\n",
    "\\end{aligned}$$\n",
    "\n",
    "- Notice pattern: $a_{l} = f_l(W_l a_{l-1} + b_l)$\n",
    "- These $a_{l}$ are the outputs of layer $l$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e378421d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLP Notation: Function Composition\n",
    "\n",
    "- Let $D_l(x; W_l, b_l) = W_l x + b_l$ -- shorthand $D_l$ (look familiar?!)\n",
    "- Let $f_l$ be *activation function* for layer $l$ (more detail soon)\n",
    "- Write MLP using function composition notation: $$y = (f_{\\text{out}} \\circ D_L \\circ f_{L-1} \\circ D_{L-1} \\circ \\cdots f_1 \\circ D_1)(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004e4ebe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Perspective\n",
    "\n",
    "- In tensorflow, we use the `keras` submodule to describe our MLP\n",
    "- The perceptron layer is defined as `keras.layers.Dense` (because the connections from one layer to the next are dense)\n",
    "- The keras code uses `keras.Sequential` and most closely follows the function composition notation from above\n",
    "- For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c105234c",
   "metadata": {},
   "outputs": [],
   "source": [
    "example_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(12, input_dim=10),  # len(x) == 10, 12 neurons in layer 1\n",
    "    tf.keras.layers.Dense(20),                # 20 neurons in layer 2\n",
    "    tf.keras.layers.Dense(10),                # 10 neurons in layer 3\n",
    "    tf.keras.layers.Dense(1)                  # one output\n",
    "])\n",
    "example_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f8d054",
   "metadata": {},
   "source": [
    "We will see many more examples of the \"code perspective\" as we build and train MLPs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63331d00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Activation Functions\n",
    "\n",
    "- Let's focus on the funcitons $f_l$ where $l=1, \\cdots L$\n",
    "- These are called activation functions\n",
    "- Key features:\n",
    "    - Nonlinear: allow MLP to model nonlinear relationships between $X$ and $y$\n",
    "    - Control range: modify output space for each layer\n",
    "    - Differentiable: allows training with gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd12bc1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Common Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a41b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.linspace(-2, 2, 30)\n",
    "\n",
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "activations = np.array([\n",
    "    [identity, tf.nn.sigmoid, tf.nn.tanh],\n",
    "    [tf.nn.relu, tf.nn.gelu, tf.nn.leaky_relu], \n",
    "    [tf.nn.softmax, tf.nn.log_softmax, tf.nn.silu],\n",
    "])\n",
    "\n",
    "fig, axs = plt.subplots(3, 3, figsize=(12, 12))\n",
    "for i in range(3):\n",
    "    for j in range(3):\n",
    "        f = activations[i, j] \n",
    "        ax = axs[i, j]\n",
    "        ax.plot(x, f(x))\n",
    "        ax.set_title(f.__name__)\n",
    "        ax.hlines(0, -2, 2, \"k\", lw=1)\n",
    "        ax.vlines(0, -2, 2, \"k\", lw=1)\n",
    "        ax.set_ylim(-2,2)\n",
    "\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75ed5b7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### So many choices!\n",
    "\n",
    "- There are *many* choices for what activation to choose\n",
    "- Our suggestion: use ReLU unless the context/model/application argues strongly that you use something else"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36c07df6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Output Activations\n",
    "\n",
    "- We have seen that we can use the MLP model for regression tasks\n",
    "- We can also use MLP for other settings\n",
    "- The key is to choose the output activation to target the proper range\n",
    "- The table below summarizes a few options\n",
    "\n",
    "| Problem                    | $Y$               | activation | output range                           |\n",
    "|----------------------------|-------------------|------------|----------------------------------------|\n",
    "| regression                 | $\\mathbb{R}^M$    | identity   | $(-\\infty, \\infty)^M$                  |\n",
    "| binary classification      | $\\{0, 1\\}$        | sigmoid    | $(0, 1)$                               |\n",
    "| multinomial classification | $\\{0, \\dots, K\\}$ | softmax    | $(0,1)^K \\; \\text{and} \\sum \\cdot = 1$ |\n",
    "| multilabel classification  | $\\{0, 1\\}^K$ | sigmoid    | $(0, 1)^K$                             |"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2890dafa",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Example: Recognizing Digits\n",
    "\n",
    "- Let's see how our MLP can be used for multinomial classification\n",
    "- The classic benchmark problem is handwriting recognition for handwritten digits\n",
    "- The dataset is known as MNIST and is referenced extensively in tutorials and other parts of the literature\n",
    "- Tensorflow has built in functions to load this data for us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94a0330",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "x_train, x_test = x_train[..., np.newaxis]/255.0, x_test[..., np.newaxis]/255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc9d426",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Data\n",
    "\n",
    "- The data is organized in 4 dimensions: (observation, pixel_x, pixel_y, color_channel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61681f20",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f23577b3",
   "metadata": {},
   "source": [
    "- In training dataset we have 60,000 observations of 28x28 grayscale images\n",
    "- We only have one color channel on a scale from 0 to 1 where 0 is white and 1 is black"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a9b11c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Visualizing the data\n",
    "\n",
    "Matplotlib can help us visualize some of the observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f7464c",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(linewidth=220, precision=1, suppress=True)\n",
    "x_train[0, :, :, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f67942d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(x_train[i, :, :, 0])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e1f2061",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Activation for Multiclass Classification\n",
    "\n",
    "- Goal: build an MLP that can label which digit is written\n",
    "- There are 10 digits (0, 1, ..., 9)\n",
    "- This is a multiclass classification problem\n",
    "- Output of model: probability distribution over 10 digits\n",
    "    - 10 element vector\n",
    "    - All elements between (0, 1)\n",
    "    - Elements sum to 1\n",
    "- Softmax activation function gives us this: $$\\text{softmax}(z)_i = \\frac{e^{z_i}}{\\sum_{k=1}^{N} e^{z_k}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "aa073749",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The right loss function\n",
    "\n",
    "- We've typically used the MSE as the loss function\n",
    "- This is appropriate for regression settings where outputs are continuous and we want to get as close to targets as possible\n",
    "- In our multi-class classification problem we want to penalize any incorrect answer\n",
    "- The most common loss function used in this setting is called the *sparse categorical cross entropy* loss function:\n",
    "$$l(y_i, \\hat{y}_i) = - \\sum_{m=0}^9 \\mathbf{1}_{y_i = m} log(\\hat{y}_{i,m})$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "59f002e5",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Intuition on loss function\n",
    "\n",
    "- $\\hat{y}_{i,m} \\in (0,1)$ tells probability with which model believes $X_i$ is digit $m$\n",
    "- $log(\\hat{y}_{i,m}) \\in (-\\infty, 0)$ is very large negative when $y_{i,m}$ close to zero, but approaaches $0$ as $y_{i,m} \\rightarrow 1$\n",
    "- Multiplying by $\\mathbf{1}_{y_i = m}$ focuses loss only on degree of incorrectness for the actual digit for this sample"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5a24e37d",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### MLP for Multiclass Classification\n",
    "\n",
    "- We are now ready to build our image classifier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6545ad70",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape[0] // 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337a4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. define model\n",
    "model = tf.keras.models.Sequential([\n",
    "  tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "  tf.keras.layers.Dense(128, activation='relu'),\n",
    "  tf.keras.layers.Dense(10, activation=\"softmax\")\n",
    "])\n",
    "\n",
    "# 2 compile model (choose optimizer and loss)\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.RMSprop(),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "    metrics=[tf.keras.metrics.SparseCategoricalAccuracy()],\n",
    ")\n",
    "\n",
    "# 3. train model\n",
    "history = model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    validation_data=(x_test, y_test),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9288ed2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(history.history).plot(figsize=(10,6))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a7edd9",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- With only one hidden layer we were able to have a test accuracy of 97.5%\n",
    "- This is quite good, but we'll see soon how we can do even better (CNNs)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76db95c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "apple-tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
