{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f07ae398",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Convolutional Neural Networks\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Tensorflow + keras\n",
    "- SGD\n",
    "- MLP\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the core concepts behind the Convolutional Neural network\n",
    "- Understand the math behind `keras.layers.Conv2d` and `keras.layers.MaxPooling2D`\n",
    "- Be able to define and fit a CNN using keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddaff73",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6da09",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Reminder: Neural Networks as Function Composition\n",
    "\n",
    "- Recall the function composition form of the multilayer perceptron model: $$y = (f_{\\text{out}} \\circ D_L \\circ f_{L-1} \\circ D_{L-1} \\circ \\cdots \\underbrace{f_1 \\circ D_1}_{\\text{key pattern}})(x)$$\n",
    "- Each function $D_l(x) = W x + b$ are called `Dense` layers and $f_l$ are activation functions\n",
    "- This same structure (composing one function after another) is common for all feed forward or sequential deep neural networks\n",
    "- Today we will learn about another type of neural network: convolutional neural networks (CNN)\n",
    "    - The `Dense` layers will be replaced by `Conv2d` plus pooling layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862328d7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Motivation: Image classification\n",
    "\n",
    "- The CNN architecture gained popularity for its use in image classification problems\n",
    "- Why? Consider MLP for images\n",
    "    - MLP connects all layer inputs to all layer outputs\n",
    "    - For image, all pixels to all neurons\n",
    "    - No spatial awareness\n",
    "- CNN, on the other hand: \n",
    "    - connects local block of layer inputs to each output\n",
    "    - Can identify localized patterns like shapes within an image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde53bb8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example Image\n",
    "\n",
    "- Consider an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33cde584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "china = datasets.load_sample_image('china.jpg')\n",
    "px.imshow(china)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9e7d68",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- In python this is represented as a 3 dimensional array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d02e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "china[:2, :2, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "810fb9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "china.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "008d4922",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Dimensions are height, width, depth\n",
    "- Here we have 3 depth dimensions for the 3 color channels red, blue, and green"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bb678e1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Local patterns in image\n",
    "\n",
    "- Consider 4 rectangular shapes from the image\n",
    "- Each has same subset of pixels in first dimension (`196:220`)\n",
    "- Each has same width in second dimension (5 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa4fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 4, figsize=(10, 6))\n",
    "\n",
    "ax[0].imshow(china[196:220, 208:213, :])\n",
    "ax[1].imshow(china[196:220, 252:257, :])\n",
    "ax[2].imshow(china[196:220, 278:283, :])\n",
    "ax[3].imshow(china[196:220, 360:365, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640aca7",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "- Three of these are similar (first 3)\n",
    "- Fourth one is very different\n",
    "- Wouldn't it be nice if...\n",
    "    - Our neural network were able to identify patterns that lead to similar activations for sub-images 1-3, but not for 4\n",
    "    - CNNs can..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa3f025c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## CNN Theory\n",
    "\n",
    "- A typical convolutional neural network might look as follows: $$y = (f_{\\text{out}} \\circ D_{\\text{out}} \\circ P_L \\circ \\cdots \\circ P_6 \\circ f_5 \\circ C_{5} \\circ f_4 \\circ C_4 \\underbrace{\\circ P_3 \\circ f_2 \\circ C_{2} \\circ f_1 \\circ C_1}_{\\text{key pattern}})(x)$$\n",
    "- $C_l$ represents a convolutional layer\n",
    "- $P_l$ represents a pooling layer\n",
    "- As with MLP, we will look at the key layers of the CNN from three perspectives:\n",
    "    - Visual\n",
    "    - Mathematical\n",
    "    - Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673ca182",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Overview\n",
    "\n",
    "- The convolutional layer works as follows:\n",
    "    - Layer inputs are processed locally\n",
    "    - Each local region (rectangle) is convolved with coefficient vector (filter or kernel) to produce a single output (convolved = dot proudct)\n",
    "    - We then \"slide\" the local region of the input to compute the next output\n",
    "    - The full output is called a feature map\n",
    "    - We can choose how many filters or feature maps appear in each conv layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8fa895",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "### Visual Perspective: A convolutional layer\n",
    "\n",
    "- The image below shows core features from a visual perspective\n",
    "\n",
    "![cnn_neuron.png](cnn_neuron.png)\n",
    "\n",
    "(Source: [Stanford cs231n](https://cs231n.github.io/convolutional-networks/))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7adb50",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "#### Visual Perspective: CNN\n",
    "\n",
    "- Many of these convolution layers can be stacked into a convolutional neural network\n",
    "- This might look as follows:\n",
    "\n",
    "![cnn_visual.png](cnn_visual.png)\n",
    "\n",
    "(Source: [KDNuggets](https://www.kdnuggets.com/2016/11/intuitive-explanation-convolutional-neural-networks.html/3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186e8c70",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Example: \"specialized filters\"\n",
    "\n",
    "- Below are some common convolutional filters (kernels) used in image processing\n",
    "- This is *not* something your CNN would likely learn, but shows you the type of processing that can be done by a convolution\n",
    "\n",
    "![cnn_filter_examples.png](cnn_filter_examples.png)\n",
    "\n",
    "(Source: [Wikipedia](https://en.wikipedia.org/wiki/Kernel_(image_processing)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a7a9de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Mathematical Perspective: Feature Map\n",
    "\n",
    "- Personally, the equations for convolutional layers are less intuitive/enlightening than the equations for the MLP\n",
    "- A treatment of the math requires non-trivial notation\n",
    "- We'll opt to not present it, but rather talk through how it works given the animation below:\n",
    "\n",
    "![cnn_ops.gif](cnn_ops.gif)\n",
    "\n",
    "(Source: [Medium](https://towardsdatascience.com/lets-code-convolutional-neural-network-in-plain-numpy-ce48e732f5d5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4d6d49",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Mathematical Perspective: Conv layer\n",
    "\n",
    "- The previous animation showed operations for a single feature map\n",
    "- In a Conv layer, there are many stacked together depth-wise\n",
    "- Each feature map has its own kernel and bias\n",
    "\n",
    "![cnn_math.png](cnn_math.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccecb8a1",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Hyperparameters\n",
    "\n",
    "- For each conv layer, we need to pick a few hyper parameters\n",
    "    - Number of filters $K$: how many items in output dimension\n",
    "    - their spatial extent $F$: How wide/tall filters are\n",
    "    - the stride $S$: how far to \"skip\" each time we slide filter\n",
    "    - the amount of zero padding $P$: chosen to make dimensions \"line up\"\n",
    "- In practice you must set $K$ and $F$, sometimes $S$, almost never $P$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bafc91",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Max Pooling\n",
    "\n",
    "- After one or more Conv+activation layers, CNNs often have a pooling layer\n",
    "- The mathematical operation is relatively simple:\n",
    "    - For each subregion of layer inputs, set the output equal to the maximum value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabd08e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Why Pooling?\n",
    "\n",
    "- Resource efficiency:\n",
    "    - Images come in w x h x 3\n",
    "    - Conv layer output is w1 x h1 x k1 -- where k1 often 32 or 64\n",
    "    - 2x2 max pooling cuts down number of items in tensor by factor of 4\n",
    "- De-noising\n",
    "    - Having very wide/tall feature maps carries abundant information\n",
    "    - In practice, might be excessive\n",
    "    - Pooling forces network to focus on most active/most relevant features \n",
    "    - Can improve performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c52912b6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Summary\n",
    "\n",
    "- Putting it all together we have a CNN similar to the following:\n",
    "\n",
    "![cnn_layers.png](cnn_layers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce69c27",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Code Perspective\n",
    "\n",
    "- Pause... breath... that was a lot!\n",
    "- The CNN has more new theory to learn, but isn't any harder to use\n",
    "- Keras has layers that we can add to a `Sequential` model to create a CNN\n",
    "\n",
    "> Note: another instance of knowing the details isn't necessary to build/use a CNN, but is extremely helpful to practice effectively + diagnose problems + understand/interpret outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4867fafe",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Our First CNN\n",
    "\n",
    "- Let's build our first CNN in keras\n",
    "- We'll apply it to the fashion MNIST image classification problem we saw before\n",
    "\n",
    "> Recall the validation accuracy we achieved using an MLP wa about 86%..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39f3a8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
    "x_train, x_test = x_train[..., None]/255.0, x_test[...,None]/255.0\n",
    "\n",
    "print(\"Number of original training examples:\", len(x_train))\n",
    "print(\"Number of original test examples:\", len(x_test))\n",
    "print(\"Shape of x_train:\", x_train.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 4, figsize=(10, 6))\n",
    "for i in range(4):\n",
    "    ax[i].imshow(x_train[i, :, :, :])\n",
    "    ax[i].set_title(y_train[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b22f2a41",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "cnn = keras.Sequential([\n",
    "    # Part 1: first (Conv)+ => Pool stack\n",
    "    # Conv layer with 32 filters of size 3x3\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\", input_shape=x_train.shape[1:]),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2, 2)),\n",
    "    \n",
    "    # Part 2: second (Conv)+ => Pool stack\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.Conv2D(32, (3, 3), activation=\"relu\"),\n",
    "    keras.layers.MaxPool2D((2, 2)),\n",
    "    \n",
    "    # Part 3: Flatten\n",
    "    keras.layers.Flatten(),\n",
    "    \n",
    "    # Part 4: Classifier\n",
    "    keras.layers.Dense(200, activation=\"relu\"),\n",
    "    \n",
    "    # Part 5: output layer\n",
    "    keras.layers.Dense(len(set(y_train)), activation=\"softmax\")\n",
    "])\n",
    "\n",
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8c9256",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "cnn.compile(optimizer='rmsprop',\n",
    "              loss=\"sparse_categorical_crossentropy\",\n",
    "              metrics=['accuracy'])\n",
    "hist = cnn.fit(x_train, y_train, validation_data=(x_test, y_test), epochs=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30dfe461",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Modern CNNs\n",
    "\n",
    "- The architecture we described was proposed by Turing award winner Yann LeCun in the late 1990s (called \"LeNet\")\n",
    "- Deep learning had little progress for the subsequent ~15 years, but exploded in the 2010s\n",
    "- Now off the shelf CNN models can achieve super (or near) human performance on a variety of image processing tasks\n",
    "- Keras has many of these models built-in\n",
    "- In the future we'll leverage the built-in models (and transfer learning!) when using CNNs for image tasks"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "css",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
