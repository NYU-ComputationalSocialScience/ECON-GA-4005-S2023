{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c938b816",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Ridge Regression\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Linear Models\n",
    "- Overfitting and Model Selection\n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand how ridge regression is a regularized version of linear regression\n",
    "- Learn how ridge regression utilizes the input data's Gram matrix\n",
    "- Be able to apply ridge regression using sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5fd79c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model\n",
    "from ipywidgets import widgets\n",
    "\n",
    "from sklearn import pipeline, linear_model, preprocessing, model_selection, metrics\n",
    "\n",
    "%matplotlib inline\n",
    "plt.style.use(\"seaborn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccbadb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## What is Ridge Regression?\n",
    "\n",
    "-   **Ridge Regression** is linear regression with an $L_2$-norm regularization\n",
    "-   This particular regularizer is conducive in obtaining a closed-form solution for the optimal weights \n",
    "\n",
    "$$\\begin{aligned}\n",
    "\\theta_{\\mu}^* & \\triangleq \\arg \\min_{\\theta} \\bigg [ \\frac{1}{N} \\| y - X \\theta  \\|_2^2 + \\frac{\\mu}{N}  \\| \\theta  \\|_2^2 \\bigg ] \\\\\n",
    "            & = \\arg \\min_{\\theta} \\frac{1}{N} \\bigg [ \\| y \\|_2^2 - 2 \\theta^T X^T y + \\theta^T \\big ( X^T X + \\mu I_P  \\big ) \\theta \\bigg ] \\\\\n",
    "            & = \\big ( X^T X + \\mu I_P \\big )^{-1} X^T y\n",
    "\\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de06d1e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Comments\n",
    "\n",
    "$$\\theta_{\\mu}^* = \\big ( \\underbrace{X^T X}_{R} + \\mu I_P \\big )^{-1} X^T y$$\n",
    "\n",
    "- Even when $R$ is not invertiable (e.g. when $P \\ge N$), $R + \\mu I_P$ always is when $\\mu > 0$\n",
    "- Finding the best $\\mu$ value $\\Longleftrightarrow$ selecting the best ridge regression model\n",
    "- $\\mu$ is hyperparameter $\\Longrightarrow$ optimal $\\mu$ chosen via validation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d4ccc2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Example: Polynomial Regression\n",
    "\n",
    "Recall our polynomial regression example\n",
    "\n",
    "Let's see what it looks like when we add and L2 regularization (i.e. use Ridge regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b8af51",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "loaded = np.load('polyreg_demo_data.npz')\n",
    "\n",
    "x = loaded['x']\n",
    "y = loaded['y']\n",
    "x_plot = loaded['x_plot']\n",
    "y_hat_plot_true = loaded['y_hat_plot']\n",
    "theta_true = loaded['theta_true']\n",
    "sigma = loaded['sigma'].item()\n",
    "N_total = x.shape[0]        # retrieve the total number of samples in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7755fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def polyreg_ridge_ridgedemo(degree=8, n_train=8, mu=0):\n",
    "    # define model\n",
    "    model = pipeline.make_pipeline(\n",
    "        preprocessing.PolynomialFeatures(degree=degree),\n",
    "        linear_model.Ridge(alpha=mu)\n",
    "    )\n",
    "    \n",
    "    X = x[:, None]  # convert to 2d\n",
    "    test_size = X.shape[0] - n_train\n",
    "    split = model_selection.train_test_split(X, y, train_size=n_train, test_size=test_size, random_state=42)\n",
    "    X_train, X_test, y_train, y_test = split\n",
    "   \n",
    "    # fit model\n",
    "    model.fit(X_train, y_train)\n",
    "    yhat = model.predict(x_plot[:, None])\n",
    "    \n",
    "    # compute metrics\n",
    "    MSE_train = metrics.mean_squared_error(y_train, model.predict(X_train))\n",
    "    MSE_holdout = metrics.mean_squared_error(y_test, model.predict(X_test))\n",
    "\n",
    "    # make the plot\n",
    "    fig = plt.figure('demo', figsize=(11, 8))\n",
    "    plt.plot(x_plot, yhat, \"k-\", lw=3, label=\"Fitted Model\")\n",
    "    plt.scatter(X_test.flatten(), y_test, color=\"b\", s=60, alpha=0.5, label=\"Hold-Out Data\")\n",
    "    plt.scatter(X_train.flatten(), y_train, color=\"r\", s=80, alpha=0.7, label=\"Training Data\") \n",
    "    plt.ylim(-20, 20)\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend(loc='upper left')\n",
    "    titletxt = 'Fitted vs. True Model Responses: mu = {} Ntrain={} deg={} MSEtrain={:.3f}, MSEholdout={:.3f}, NoiseVar={:.3f}'\n",
    "    plt.title(titletxt.format(mu, n_train, degree, MSE_train, MSE_holdout, sigma))\n",
    "    \n",
    "    return fig\n",
    "\n",
    "\n",
    "widgets.interactive(\n",
    "    polyreg_ridge_ridgedemo, \n",
    "    degree=(1, 10, 1), \n",
    "    n_train=(1, 35, 1),\n",
    "    mu=(0, 10.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99051ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gram Matrix\n",
    "\n",
    "- Definition: for a collection of $N$ vectors $\\{X_n \\}_{n=1}^N$ their Gram matrix $G$ is defined as the matrix containing all possible dot products between them:\n",
    "$$G \\triangleq XX^T = \\begin{bmatrix}\n",
    "x_1^Tx_1 & x_1^Tx_2 & \\cdots \\\\\n",
    "x_2^Tx_1 & x_2^Tx_2 & \\cdots \\\\\n",
    "\\dots & \\dots & \\ddots\n",
    "\\end{bmatrix} \\in \\mathbb{R}^{N\\times N}$$\n",
    "- Symmetry: the dot product is symmetric in its arguments, therefore $G$ is symmetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1096e6",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Ridge Regression via Gram Matrix\n",
    "\n",
    "- For suitable matrices (conformable and invertible as needed), the following holds:\n",
    "$$\\big(A + B^TCB \\big)^{-1} = A^{-1}B^T (B A^{-1}B^T + C^{-1})^{-1}$$\n",
    "- When applied to $(R + \\mu I_P)^{-1} = (X^TX + \\mu I_P)^{-1}$ for $\\mu > 0$ one obtains: $$(R + \\mu I_P)^{-1} = X^T (G + \\mu I_N)^{-1}$$\n",
    "- Therefore: $$\\theta_{\\mu}^* = X^T (G + \\mu I_N)^{-1} y$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f445b39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Ridge Regression by Hand\n",
    "\n",
    "Let's use this theory to compute ridge regression \"by hand\"\n",
    "\n",
    "We know that we can find $\\theta$ via the Gram matrix: $$\\theta_{\\mu}^* = X^T (G + \\mu I_N)^{-1} y$$\n",
    "\n",
    "We also know we can find it directly: $$\\theta_{\\mu}^* = \\big ( X^T X + \\mu I_P \\big )^{-1} X^T y$$\n",
    "\n",
    "We'll show both approaches below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "088f1aa2",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "n_train = 10\n",
    "n_test = len(y) - n_train\n",
    "\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    x[:, None], y, test_size=n_test, train_size=n_train, random_state=12\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0a97c8",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "D = 8\n",
    "poly = preprocessing.PolynomialFeatures(degree=D)\n",
    "X = poly.fit_transform(X_train)\n",
    "G = X@X.T\n",
    "N, P = X.shape\n",
    "IP = np.eye(P)\n",
    "IN = np.eye(N)\n",
    "\n",
    "mus = [0.005, 0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0, 200.0, 1000.0]\n",
    "\n",
    "thetas = np.zeros((P, len(mus)))\n",
    "thetas_gram = np.zeros((P, len(mus)))\n",
    "\n",
    "for i, mu in enumerate(mus):\n",
    "    thetas[:, i] = np.linalg.inv(X.T@X + mu*IP) @ (X.T @ y_train)\n",
    "    thetas_gram[:, i] = X.T @ np.linalg.inv(G + mu*IN) @ y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "197c0356",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's verify that we got the same answer using our two approaches:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703a34f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(thetas - thetas_gram).max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bc1f92",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "That's very close to zero, especially when you consider the numerical inverses we were computing, so we'll call it a success."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d3c96f",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Given the loss function we minimized, we should see that as $\\mu$ increases (we move to columns further right in `ws`), the coefficients in `ws` get smaller:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993e92dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(thetas, 2, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc0e2e3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Excellent!\n",
    "\n",
    "Recall that the reason we penalize the coefficients in this manner is to regularize the model in the hope that it will have better generalization properties\n",
    "\n",
    "Let's see if this holds in practice this claim by computing the hold-out MSE for each set of coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eb16d58",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "N_mu = len(mus)\n",
    "test_MSEs = np.zeros(N_mu)\n",
    "Xtest = poly.transform(X_test)\n",
    "for i in range(N_mu):\n",
    "    test_MSEs[i] = metrics.mean_squared_error(y_test, Xtest @ thetas[:, i])\n",
    "    \n",
    "    msg = \"The holdout MSE when mu = {:.3f} is {:.4f}\"\n",
    "    print(msg.format(mus[i], test_MSEs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e5d88c",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "Let's plot the holdout MSE as a function of $\\mu$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6728c358",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "ax.semilogx(mus, test_MSEs, \"b.-\", ms=15)\n",
    "ax.set_xlabel(\"$\\mu$ (notice the log scale)\")\n",
    "ax.set_ylabel(\"$MSE_{\\text{test}}$\")\n",
    "ax.set_title(\"Regularization parameter $\\mu$ vs. holdout MSE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64431b5e",
   "metadata": {},
   "source": [
    "Notice that the holdout MSE has a u-shaped curve as a function of the regularization parameter\n",
    "\n",
    "What you are seeing is that as $\\mu \\downarrow 0$ we approach the linear un-regularized case, which is pure linear regression\n",
    "\n",
    "In this case the validation error will be quite large because the model is overfitting\n",
    "\n",
    "As $\\mu \\uparrow \\infty$ the model is being constrained to have very small weights and is underfitting\n",
    "\n",
    "$\\mu$ is a hyperparameter that needs to be chosen by a validation procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b825427a",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "If we were choosing between the models indexed by our list of $\\mu$ values and were applying the hold-out validation proceedure, we would choose the third model as our champion model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ded0fbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"The champion model has mu = {}\".format(mus[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d11eda85",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Summary\n",
    "\n",
    "- Ridge regression is an extension of linear regression with L2-regularization\n",
    "- The objective function is: $MSE(\\theta; x, y) + \\mu \\|\\theta\\|_2^2$\n",
    "- As $\\mu$ increases, the parameters are all *compressed* towards zero in absolute value\n",
    "- If $\\mu$ is small and model is overparameterized, overfitting may occur\n",
    "- If $\\mu$ is too large, underfitting will occur"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
