{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "# Linear Models II,  GLM\n",
    "\n",
    "**Prerequisites**\n",
    "\n",
    "- Linear Algebra\n",
    "- Calculus\n",
    "- Some probability/statistics\n",
    "- Linear Models 1\n",
    "\n",
    "\n",
    "**Outcomes**\n",
    "\n",
    "- Understand the generalized linear model (GLM)\n",
    "- Apply GLM to binary, count, and severity targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "#%pip install --user pyarrow python-snappy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.formula.api as smf\n",
    "import statsmodels.api as sm\n",
    "from sklearn import linear_model\n",
    "import seaborn as sns\n",
    "import scipy.stats as st\n",
    "\n",
    "from IPython.display import set_matplotlib_formats\n",
    "plt.style.use(\"seaborn\")\n",
    "%matplotlib inline\n",
    "set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generalized Linear Models (GLMs)\n",
    "\n",
    "Extension of the linear regression model to additional settings, such as:\n",
    "\n",
    "-   Binary data (logistic regression)\n",
    "-   Count data (Poisson regression)\n",
    "-   Variance of predictor should vary with mean of predictor\n",
    "    (Gamma regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Standard Linear Regression\n",
    "\n",
    "-   The standard linear regression model can be written $$y_i = x_i^T\n",
    "           \\theta + \\epsilon_i$$\n",
    "-   The standard assumption is that $\\epsilon_i \\stackrel{\\text{iid}}{\\sim}\n",
    "           N(0, \\sigma^2)$\n",
    "-   Under this assumption, three are three key features of the model:\n",
    "    1.  The mean of predictions $\\mu_i = E[y_i | x_i]$ depends on the linear\n",
    "        predictor $\\eta_i {\\color{blue} \\triangleq} x_i^T \\theta$\n",
    "    2.  The distribution of predictions is $y_i | x_y \\sim N(\\mu_i, \\sigma^2)$\n",
    "    3.  The expected response is equal to linear predictor $\\mu_i = \\eta_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GLM: Links and Families\n",
    "\n",
    "-   The **generalized linear model** loosens restrictions 2 and 3\n",
    "-   The distribution of predictions $y_i | x_i$ can come from the family of\n",
    "    exponential dispersion models: $$y_i | x_i \\sim f_{EDM}(y_i | x_i, \\beta, \\phi)\n",
    "           = \\exp \\left(\\frac{y \\beta -  b(\\beta)}{\\phi} + c(y; \\phi) \\right)$$\n",
    "    -   $\\beta$ is called the **canonical parameter** and depends on the linear\n",
    "        predictor $\\eta$\n",
    "    -   $\\phi$ is called the **dispersion parameter**\n",
    "-   Relaxing assumption 3, we also relate the expected response to the linear\n",
    "    prediction via a function $g$ called the **link function**: $$g(\\mu_i) = \\eta_i$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### GLM: 3 key assumptions\n",
    "\n",
    "-   Restating assumptions 1-3 for the GLM, we have\n",
    "    1.  The mean of predictions $\\mu_i = E[y_i | x_i]$ depends on the linear\n",
    "        predictor $\\eta_i \\triangleq x_i^T \\theta$\n",
    "    2.  The distribution of predictions is $y_i | x_i \\sim f_{EDM}(y_i |x_i, \\beta,\n",
    "                  \\phi)$\n",
    "    3.  The expected response related to the linear predictor my a monotonic\n",
    "        link function $g$: $g(\\mu_i) = \\eta_i$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: Linear Regression Model\n",
    "\n",
    "-   Recall the pdf of the Normal distribution: $$f(y; \\mu, \\sigma) =\n",
    "           \\frac{1}{\\sigma \\sqrt{2 \\pi}} \\exp \\left(\\frac{(y - \\mu)^2}{2 \\sigma^2} \\right)$$\n",
    "-   The standard linear regression model is a special of the GLM where\n",
    "    -   Link function is $g(\\mu) = \\mu$ (the identity function)\n",
    "    -   The parameter $\\phi = \\sigma^2$\n",
    "    -   The parameter $\\beta = \\mu$\n",
    "    -   $b(\\beta) = \\beta^2 / 2 = \\mu^2 / 2$\n",
    "    -   $c(y, \\phi) = -1/2 \\left[y^2/\\sigma^2 + \\log(2 \\pi \\sigma^2) \\right]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Example: Logistic Regression\n",
    "\n",
    "-   The logistic regression model is also a special case of the GLM\n",
    "-   To see it, start with $Y \\sim Bernoulli(p)$ (note $E[Y] = \\mu = p$)\n",
    "-   The pmf for the Bernoulli distribution is: $$f(y; p) = p^y(1-p)^{1-y}$$\n",
    "-   The GLM is then specified using\n",
    "    $$\\begin{aligned}\n",
    "           g(\\mu) &= \\log \\left(\\frac{\\mu}{1-\\mu} \\right) \\\\\n",
    "           \\phi &= 1 \\\\\n",
    "           \\beta &= \\log \\left(\\frac{\\mu}{1-\\mu} \\right) \\\\\n",
    "           b(\\beta) &= \\log(1 + \\exp(\\beta)) \\\\\n",
    "           c(y, \\phi) &= 1/y\n",
    "           \\end{aligned}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Note\n",
    "\n",
    "-   Most of this math is taken care of for us \"behind the scenes\"\n",
    "-   In practice, when using the GLM framework we do the following:\n",
    "    1.  Specify the linear predictor $\\eta_i = x_i^T \\theta$\n",
    "    2.  Choose the family of the regression (either Gaussian for linear\n",
    "        regression and Binomial/Bernoulli for logistic regression)\n",
    "    3. Optionally choose a suitable link function, though often the default link per family is a good choice"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- The implementation/library will take care of parameter estimation\n",
    "-   We cover the math here so that we have a sense of the assumptions we are\n",
    "    making when we choose a particular model\n",
    "-   This helps us choose the right model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## GLM Families\n",
    "\n",
    "- The GLM framework can handle many use cases:\n",
    "    - Continuous, real-valued outputs (linear regresion)\n",
    "    - Binary outcomes (logistic regression)\n",
    "    - Count data (Poisson regression)\n",
    "    - Average severity of event (Gamma regression)\n",
    "    - Total severity of all events (Tweedie regression)\n",
    "- We've learned about linear and logistic regression already\n",
    "- We'll now study the other cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### A working example\n",
    "\n",
    "-   As we study, we will work on a running example\n",
    "-   This example is adapted from a sklearn tutorial [here](https://scikit-learn.org/stable/auto_examples/linear_model/plot_tweedie_regression_insurance_claims.html#sphx-glr-download-auto-examples-linear-model-plot-tweedie-regression-insurance-claims-py) (which came from a\n",
    "    paper [here](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764))\n",
    "-   Data: Data for 678,013 auto insurance policies and associated claims from private auto insurer in France\n",
    "-   Features: vehicle age, vehicle power, driver age, bonus malus (extra premium for bad driving history), population density in city where driver lives\n",
    "-   Target(s): number of claims, amount of money per claim, total expected claim over life of policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Data Features\n",
    "\n",
    "Let's load up the dataset and take a look at the feature space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"https://css-materials.s3.amazonaws.com/ML/linear_models_2/insurance_claims_data.parquet\")\n",
    "df.info()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Note:**\n",
    "\n",
    "- A few columns are derived/computed:\n",
    "    - `PurePremium = ClaimAmount/Exposure`: the insurance company's expected total claim per unit of exposure\n",
    "    - `Frequency = ClaimNb/Exposure`: the number of claims per unit of exposure (year)\n",
    "    - `AvgClaimAmount = ClaimAmount/ClaimNb`: average claim value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 2, figsize=(12, 4))\n",
    "df[[\"Exposure\"]].plot.hist(ax=ax[0,0])\n",
    "df[[\"Exposure\"]].plot.box(ax=ax[0,1])\n",
    "df[[\"ClaimNb\"]].plot.hist(ax=ax[1,0])\n",
    "sns.ecdfplot(df[[\"AvgClaimAmount\"]].query(\"AvgClaimAmount > 0\"))\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Split data\n",
    "\n",
    "- We will split the dataset into a training and a testing subset\n",
    "- This will allow us to see test the out of sample performance of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=50000)\n",
    "\n",
    "# copy dataframe to avoid warnings later\n",
    "df_train = df_train.copy()\n",
    "df_test = df_test.copy()  \n",
    "\n",
    "print(f\"Training dataset has {df_train.shape[0]} rows and testing has {df_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Objective\n",
    "\n",
    "- Our objective with this dataset will be to play role of insurer and model the PurePremium amount based on driver characteristics\n",
    "- We have two approaches:\n",
    "    1. Model the expected number of claims (Poisson) and averge claim amount (Gamma), then multiply the two estimates together\n",
    "    2. Model total claim amount directly using a Tweedie regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": "true",
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Poisson\n",
    "\n",
    "- The Poisson distribution is a discrete distribution characterizing the number of occurances of an event ($y$) in a fixed amount of time, given the average or expected number of events ($\\mu$)\n",
    "- The distribution has one parameter $\\mu$ and has pmf: $$f(y;\\mu) = \\frac{\\mu^y e^{-\\mu}}{y!} = \\exp(y \\log \\mu - \\mu - \\log y!)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    },
    "tags": []
   },
   "source": [
    "- This has the form of our exponential family distribution with\n",
    "$$\\begin{aligned}\n",
    "\\phi &= 1 \\\\\n",
    "\\beta &= \\log \\mu\\\\\n",
    "b(\\beta) &= \\exp(\\beta) = \\mu \\\\\n",
    "c(y, \\phi) &= - \\log y!\n",
    "\\end{aligned}$$\n",
    "- Because $E[y] = \\mu > 0$, we must have a link function that maps from $(-\\infty, \\infty) \\rightarrow (0, \\infty)$\n",
    "- A natural choice for the link function is the log function $g(\\mu) = \\log(\\mu)$, which implies $E[y_i|x_i] =\\mu_i = \\exp(x_i^T \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Poisson PMF\n",
    "\n",
    "- Let's take a look at the pmf for the Poisson distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_pmf(dist, lb=0, ub=20, ax=None, **kw):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    x = np.arange(lb, ub)\n",
    "    ax.plot(x, dist.pmf(x), \"*-\")\n",
    "    return ax\n",
    "    \n",
    "fig, ax = plt.subplots()\n",
    "λs = [1, 4, 10]\n",
    "for λ in λs:\n",
    "    plot_pmf(st.poisson(λ), ax=ax)\n",
    "ax.legend([f\"λ = {λ}\" for λ in λs], loc=\"upper right\") \n",
    "ax.set_xlabel(\"y\")\n",
    "ax.set_ylabel(r\"$f_{\\lambda}(y)$\")\n",
    "ax.set_title(\"Poisson distribution\")\n",
    "ax.text(10, 0.15, \"E[Y | λ] = Var(Y | λ) = λ\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Poisson Claims\n",
    "\n",
    "- The GLM where the expected response is Poisson distributed is known as Poisson regression\n",
    "- To apply Poisson regression to our claims data, our response variable will be the frequency of claims\n",
    "- The model we will use is $$\n",
    "\\begin{aligned} \n",
    "E[y_i | x_i] &= \\mu_i = \\exp(x_i^T \\theta) \\text{, with} \\\\ \n",
    "x_i &= \\begin{bmatrix}1 \\\\ \\text{VehPower}_i \\\\ \\text{VehAge}_i \\\\ \\text{DrivAge}_i \\\\ \\text{BonusMalus}_i \\\\ \\log(\\text{Density})_i \\end{bmatrix}\n",
    "\\end{aligned}$$\n",
    "\n",
    "- We will also weight each observation by its `Exposure` to encourage the model to pay attention to high exposure policies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "for _df in [df_train, df_test]:\n",
    "    _df[\"log_Density\"] = np.log(_df[\"Density\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Fitting frequency model\n",
    "\n",
    "- We can use either statsmodels or sklearn to fit our Poisson regression\n",
    "- We'll start with statsmodels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "freq_mod = smf.glm(\n",
    "    \"Frequency ~ VehPower + VehAge + DrivAge + BonusMalus + log_Density\", \n",
    "    data=df_train, \n",
    "    family=sm.families.Poisson(),\n",
    "    var_weights=df_train[\"Exposure\"]\n",
    ")\n",
    "freq_fit = freq_mod.fit()\n",
    "freq_fit.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "source": [
    "### Visualizing results\n",
    "\n",
    "- To help us make sense of these results, we will use a function from the sklearn tutorial above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_obs_pred(df, feature, weight, observed, predicted, y_label=None,\n",
    "                  title=None, ax=None, fill_legend=False):\n",
    "    \"\"\"Plot observed and predicted - aggregated per feature level.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : DataFrame\n",
    "        input data\n",
    "    feature: str\n",
    "        a column name of df for the feature to be plotted\n",
    "    weight : str\n",
    "        column name of df with the values of weights or exposure\n",
    "    observed : str\n",
    "        a column name of df with the observed target\n",
    "    predicted : DataFrame\n",
    "        a dataframe, with the same index as df, with the predicted target\n",
    "    fill_legend : bool, default=False\n",
    "        whether to show fill_between legend\n",
    "    \"\"\"\n",
    "    # aggregate observed and predicted variables by feature level\n",
    "    df_ = df.loc[:, [feature, weight]].copy()\n",
    "    df_[\"observed\"] = df[observed] * df[weight]\n",
    "    df_[\"predicted\"] = predicted * df[weight]\n",
    "    df_ = (\n",
    "        df_.groupby([feature])[[weight, \"observed\", \"predicted\"]]\n",
    "        .sum()\n",
    "        .assign(observed=lambda x: x[\"observed\"] / x[weight])\n",
    "        .assign(predicted=lambda x: x[\"predicted\"] / x[weight])\n",
    "    )\n",
    "\n",
    "    ax = df_.loc[:, [\"observed\", \"predicted\"]].plot(style=\".\", ax=ax)\n",
    "    y_max = df_.loc[:, [\"observed\", \"predicted\"]].values.max() * 0.8\n",
    "    p2 = ax.fill_between(\n",
    "        df_.index,\n",
    "        0,\n",
    "        y_max * df_[weight] / df_[weight].values.max(),\n",
    "        color=\"g\",\n",
    "        alpha=0.1,\n",
    "    )\n",
    "    if fill_legend:\n",
    "        ax.legend([p2], [\"{} distribution\".format(feature)])\n",
    "    ax.set(\n",
    "        ylabel=y_label if y_label is not None else None,\n",
    "        title=title if title is not None else \"Train: Observed vs Predicted\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_model_outcomes(fit, df_train, df_test, observed, weight=\"Exposure\"):\n",
    "    fig, ax = plt.subplots(ncols=2, nrows=3, figsize=(12, 8))\n",
    "\n",
    "    plot_obs_pred(\n",
    "        df=df_train,\n",
    "        feature=\"DrivAge\",\n",
    "        weight=weight,\n",
    "        observed=observed,\n",
    "        predicted=fit.predict(df_train),\n",
    "        y_label=observed,\n",
    "        title=\"train data\",\n",
    "        ax=ax[0, 0],\n",
    "    )\n",
    "\n",
    "    plot_obs_pred(\n",
    "        df=df_test,\n",
    "        feature=\"DrivAge\",\n",
    "        weight=weight,\n",
    "        observed=observed,\n",
    "        predicted=fit.predict(df_test),\n",
    "        y_label=observed,\n",
    "        title=\"test data\",\n",
    "        ax=ax[0, 1],\n",
    "        fill_legend=True\n",
    "    )\n",
    "\n",
    "    plot_obs_pred(\n",
    "        df=df_test,\n",
    "        feature=\"VehAge\",\n",
    "        weight=weight,\n",
    "        observed=observed,\n",
    "        predicted=fit.predict(df_test),\n",
    "        y_label=observed,\n",
    "        title=\"test data\",\n",
    "        ax=ax[1, 0],\n",
    "        fill_legend=True\n",
    "    )\n",
    "\n",
    "    plot_obs_pred(\n",
    "        df=df_test,\n",
    "        feature=\"BonusMalus\",\n",
    "        weight=weight,\n",
    "        observed=observed,\n",
    "        predicted=fit.predict(df_test),\n",
    "        y_label=observed,\n",
    "        title=\"test data\",\n",
    "        ax=ax[1, 1],\n",
    "        fill_legend=True\n",
    "    )\n",
    "\n",
    "    plot_obs_pred(\n",
    "        df=df_test,\n",
    "        feature=\"VehPower\",\n",
    "        weight=weight,\n",
    "        observed=observed,\n",
    "        predicted=fit.predict(df_test),\n",
    "        y_label=observed,\n",
    "        title=\"test data\",\n",
    "        ax=ax[2, 0],\n",
    "        fill_legend=True\n",
    "    )\n",
    "\n",
    "    plot_obs_pred(\n",
    "        df=df_test,\n",
    "        feature=\"log_Density\",\n",
    "        weight=weight,\n",
    "        observed=observed,\n",
    "        predicted=fit.predict(df_test),\n",
    "        y_label=observed,\n",
    "        title=\"test data\",\n",
    "        ax=ax[2, 1],\n",
    "        fill_legend=True\n",
    "    )\n",
    "    fig.tight_layout();\n",
    "    return fig\n",
    "    \n",
    "\n",
    "plot_model_outcomes(freq_fit, df_train, df_test, observed=\"Frequency\", weight=\"Exposure\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments:\n",
    "\n",
    "- Claim frequency is very high for drivers younger than 25 and older than 80\n",
    "- As BonusMalus rises, so do claims\n",
    "- The relationshp between log(Density) and claim frequency is positive, but our model doesn't capture much of the variation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Gamma\n",
    "\n",
    "- The Gamma distribution is a continuous distribution with support in $\\mathbb{R}^{+}$\n",
    "- The distribution has two parameters: $\\mu, \\nu \\in \\mathbb{R}^{+}$ and has pdf: $$f_{\\Gamma}(y; \\mu, \\nu) = \\frac{1}{\\Gamma(\\nu)} \\left(\\frac{\\mu}{\\mu}\\right)^{\\nu}y^{\\nu-1} e^{-\\frac{\\nu}{\\mu} y}$$\n",
    "- With some heroic algebraic manipulation, $f_{\\Gamma}$ can be put into the form of $f_{EDM}$ (trust us 😉)\n",
    "- Because the suppport of the Gamma distribution is $\\mathbb{R}^{+}$, we need a link function that can map from $(-\\infty, \\infty)$ to $(0, \\infty)$\n",
    "- The most common link function is again the log link, which implies $E[y_i|x_i] =\\mu_i = \\exp(x_i^T \\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing $f_{\\Gamma}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_pdf(dist, lb=0, ub=20, ax=None, **kw):\n",
    "    if ax is None:\n",
    "        _, ax = plt.subplots()\n",
    "    x = np.linspace(lb, ub, 100)\n",
    "    ax.plot(x, dist.pdf(x), **kw)\n",
    "    return ax\n",
    "    \n",
    "fig, ax = plt.subplots(nrows=2, ncols=1, figsize=(8,6))\n",
    "μνs = [(0.5, 1), (1, 1), (2,1), (2.5, 0.5), (2.5, 1), (2.5,3)]\n",
    "for μ in [0.5, 1, 2]:\n",
    "    ν = 1  # fixed\n",
    "    label = f\"μ={μ}, ν={ν}\"\n",
    "    plot_pdf(st.gamma(μ, scale=μ/ν), ax=ax[0], label=label)\n",
    "    \n",
    "for ν in [0.5, 1, 3]:\n",
    "    μ = 2.5  # fixed\n",
    "    label = f\"μ={μ}, ν={ν}\"\n",
    "    plot_pdf(st.gamma(μ, scale=μ/ν), ax=ax[1], label=label)\n",
    "\n",
    "for _ax in ax:\n",
    "    \n",
    "    _ax.legend(loc=\"upper right\")\n",
    "    _ax.set_xlabel(\"y\")\n",
    "    _ax.set_ylabel(r\"$f_{\\Gamma}(y)$\")\n",
    "\n",
    "ax[0].set_title(\"Gamma distribution (ν = 1)\")\n",
    "ax[1].set_title(\"Gamma distribution (μ = 2.5)\")\n",
    "ax[0].text(5, 0.2, \"E[y|μ,ν] = μ, Var(y|μ,ν)= μ^2/ν\")\n",
    "fig.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comments\n",
    "\n",
    "- The parameter $\\mu$ dictates shape: notice $\\mu \\le 1$ vs $\\mu > 1$\n",
    "- $\\mu$ has stronger impact on spread of distribution ($\\text{Var} = \\mu^2/\\nu$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gamma regression\n",
    "\n",
    "- The GLM where the expected response is Gamma distributed is called Gamma regression\n",
    "- As noted above, the expected value of the model prediction is $E[y_i | x_i] = \\mu_i = \\exp(x_i^T \\theta)$\n",
    "- The variance of the prediciton is $\\text{Var}(y_i | x_i) = \\mu_i^2 / \\nu$\n",
    "- Note that the variance now changes with the mean\n",
    "- This means that Gamma regression is appropriate when the variation in the prediction should increase with the mean value of the prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Gamma claims\n",
    "\n",
    "- We will use Gamma regression to model the average claim amount\n",
    "- The implication of the variance depending on the mean makes sense in this setting\n",
    "    - Suppose $y_i$ = \\$100$. This would come from minor accident. Variance of claim amount for minor damage is small\n",
    "    \n",
    "    - Alternatively, suppose $y_i = \\$10,000$. This would be a larger accident. Large accidents likely have higher variance/dispersion\n",
    "- We will use the same set of regressors as with the Poisson model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# note: support of gamma is (0, ∞), not [0, ∞)\n",
    "# we need to filter out zero claims\n",
    "df_train_pos = df_train.query(\"ClaimAmount > 0\")\n",
    "df_test_pos = df_test.query(\"ClaimAmount > 0\")\n",
    "\n",
    "sev_mod = smf.glm(\n",
    "    \"AvgClaimAmount ~ VehPower + VehAge + DrivAge + BonusMalus + log_Density\", \n",
    "    data=df_train_pos,\n",
    "    family=sm.families.Gamma(link=sm.families.links.log()), \n",
    "    var_weights=df_train_pos[\"ClaimNb\"]\n",
    ")\n",
    "sev_fit = sev_mod.fit()\n",
    "sev_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_outcomes(sev_fit, df_train_pos, df_test_pos, observed=\"AvgClaimAmount\", weight=\"ClaimNb\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Combining Poisson + Gamma\n",
    "\n",
    "- We now have models for :\n",
    "    - `Frequency ~  Poisson` (Frequency = ClaimNb/Exposure)\n",
    "    - `AvgClaimAmount ~  Gamma` (AvgClaimAmount = ClaimAmount/ClaimNb)\n",
    "- Our goal is to predict `PurePremium = ClaimAmount/Exposure`\n",
    "- We can multiply the prediction from the two models to approximate the PurePremium "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class FreqAvgProduct:\n",
    "    def __init__(self, freq_fit, sev_fit):\n",
    "        self.freq_fit = freq_fit\n",
    "        self.sev_fit = sev_fit\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.freq_fit.predict(X) * self.sev_fit.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "product_fit = FreqAvgProduct(freq_fit, sev_fit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_outcomes(product_fit, df_train, df_test, observed=\"PurePremium\", weight=\"Exposure\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "## Tweedie\n",
    "\n",
    "- The final GLM we will consider today is called the Tweedie GLM\n",
    "- We won't cover the details of the Tweedie distribution here\n",
    "- Instead, we'll note that when the Tweedie parameter $p$ is between 1 and 2, the Tweedie GLM acts like the compound Poisson Gamma model we just computed (compound model is special case of Tweedie GLM)\n",
    "- We'll use a value of $p = 1.9$ and have statsmodels fit a Tweedie GLM to target the `PurePremium` directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "pure_mod = smf.glm(\n",
    "    \"PurePremium ~ VehPower + VehAge + DrivAge + BonusMalus + log_Density\", \n",
    "    data=df_train,\n",
    "    family=sm.families.Tweedie(link=sm.families.links.log(), var_power=1.9), \n",
    "    var_weights=df_train[\"Exposure\"]\n",
    ")\n",
    "pure_fit = pure_mod.fit()\n",
    "pure_fit.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "plot_model_outcomes(pure_fit, df_train, df_test, observed=\"PurePremium\", weight=\"Exposure\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ordered Lorenz Curve\n",
    "\n",
    "- We'll wrap up our analysis by comparing the two models for `PurePremium` against an oracle that can see the future\n",
    "- We'll construct a plot of cumulative claims where...\n",
    "    - Policy holders are ranked from safest to riskiest (order by `PurePremium`) and laid out on x-axis\n",
    "    - Fraction of observed total claims plotted on y-axis\n",
    "- This follows the sklearn tutorial and is best understood by seeing it, so let's dive in!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import auc\n",
    "\n",
    "def lorenz_curve(y_true, y_pred, exposure):\n",
    "    y_true, y_pred = np.asarray(y_true), np.asarray(y_pred)\n",
    "    exposure = np.asarray(exposure)\n",
    "\n",
    "    # order samples by increasing predicted risk:\n",
    "    ranking = np.argsort(y_pred)\n",
    "    ranked_exposure = exposure[ranking]\n",
    "    ranked_pure_premium = y_true[ranking]\n",
    "    cumulated_claim_amount = np.cumsum(ranked_pure_premium * ranked_exposure)\n",
    "    cumulated_claim_amount /= cumulated_claim_amount[-1]\n",
    "    cumulated_samples = np.linspace(0, 1, len(cumulated_claim_amount))\n",
    "    return cumulated_samples, cumulated_claim_amount\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "y_pred_product = freq_fit.predict(df_test) * sev_fit.predict(df_test)\n",
    "y_pred_total = pure_fit.predict(df_test)\n",
    "\n",
    "for label, y_pred in [(\"Frequency * Severity model\", y_pred_product),\n",
    "                      (\"Compound Poisson Gamma\", y_pred_total)]:\n",
    "    ordered_samples, cum_claims = lorenz_curve(\n",
    "        df_test[\"PurePremium\"], y_pred, df_test[\"Exposure\"])\n",
    "    gini = 1 - 2 * auc(ordered_samples, cum_claims)\n",
    "    label += \" (Gini index: {:.3f})\".format(gini)\n",
    "    ax.plot(ordered_samples, cum_claims, linestyle=\"-\", label=label)\n",
    "\n",
    "# Oracle model: y_pred == y_test\n",
    "ordered_samples, cum_claims = lorenz_curve(\n",
    "    df_test[\"PurePremium\"], df_test[\"PurePremium\"], df_test[\"Exposure\"])\n",
    "gini = 1 - 2 * auc(ordered_samples, cum_claims)\n",
    "label = \"Oracle (Gini index: {:.3f})\".format(gini)\n",
    "ax.plot(ordered_samples, cum_claims, linestyle=\"-.\", color=\"gray\",\n",
    "        label=label)\n",
    "\n",
    "# Random baseline\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", color=\"black\",\n",
    "        label=\"Random baseline\")\n",
    "ax.set(\n",
    "    title=\"Lorenz Curves\",\n",
    "    xlabel=('Fraction of policyholders\\n'\n",
    "            '(ordered by model from safest to riskiest)'),\n",
    "    ylabel='Fraction of total claim amount'\n",
    ")\n",
    "ax.legend(loc=\"upper left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "- The generalized linear model extends the linear regression framework to settings beyond the case of normally distributed responses with constant variance\n",
    "- The GLM is composed of\n",
    "    1. A linear prediction $\\eta_i =x_i^T \\theta$\n",
    "    2. A family $y_i | x_i \\sim f$\n",
    "    3. A link function $g(\\mu_i) = \\eta_i$\n",
    "- The table below summarizes the subtypes of GLM we have learned about:\n",
    "\n",
    "| Response type | Family | Link |\n",
    "| ------------- | ------ | ---- |\n",
    "| continuous | gaussian | identity |\n",
    "| binary | binomial/bernoulli | logit |\n",
    "| count | poisson | log |\n",
    "| average severity | gamma | log |\n",
    "| total severity | tweedie | log |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "CSS",
   "language": "python",
   "name": "css"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
